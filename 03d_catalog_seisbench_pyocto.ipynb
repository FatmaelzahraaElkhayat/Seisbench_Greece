{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "41639cfb",
      "metadata": {
        "id": "41639cfb"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seisbench/seisbench/blob/main/examples/03d_catalog_seisbench_pyocto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb323ba8",
      "metadata": {
        "id": "fb323ba8"
      },
      "source": [
        "![image](https://raw.githubusercontent.com/seisbench/seisbench/main/docs/_static/seisbench_logo_subtitle_outlined.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c51a8f4",
      "metadata": {
        "id": "5c51a8f4"
      },
      "source": [
        "*This code is necessary on colab to install SeisBench and the other required dependencies. If all dependencies are already installed on your machine, you can skip this.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6fdf18c2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fdf18c2",
        "outputId": "64ca60ab-cd38-4475-8825-62e2984cc46b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seisbench in /usr/local/lib/python3.11/dist-packages (0.9.1)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from seisbench) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.1 in /usr/local/lib/python3.11/dist-packages (from seisbench) (2.2.2)\n",
            "Requirement already satisfied: h5py>=3.1 in /usr/local/lib/python3.11/dist-packages (from seisbench) (3.14.0)\n",
            "Requirement already satisfied: obspy>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from seisbench) (1.4.2)\n",
            "Requirement already satisfied: tqdm>=4.52 in /usr/local/lib/python3.11/dist-packages (from seisbench) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from seisbench) (2.6.0+cu124)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from seisbench) (1.16.0)\n",
            "Requirement already satisfied: nest_asyncio>=1.5.3 in /usr/local/lib/python3.11/dist-packages (from seisbench) (1.6.0)\n",
            "Requirement already satisfied: bottleneck>=1.3 in /usr/local/lib/python3.11/dist-packages (from seisbench) (1.4.2)\n",
            "Requirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.11/dist-packages (from obspy>=1.3.1->seisbench) (3.10.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from obspy>=1.3.1->seisbench) (5.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from obspy>=1.3.1->seisbench) (75.2.0)\n",
            "Requirement already satisfied: sqlalchemy<2 in /usr/local/lib/python3.11/dist-packages (from obspy>=1.3.1->seisbench) (1.4.54)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from obspy>=1.3.1->seisbench) (4.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from obspy>=1.3.1->seisbench) (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1->seisbench) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1->seisbench) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1->seisbench) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->seisbench) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->seisbench) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->seisbench) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->seisbench) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->seisbench) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->seisbench) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->seisbench) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->seisbench) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->seisbench) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->seisbench) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->seisbench) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->seisbench) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->seisbench) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->seisbench) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->seisbench) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->seisbench) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->seisbench) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->seisbench) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->seisbench) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->seisbench) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10.0->seisbench) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->obspy>=1.3.1->seisbench) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->obspy>=1.3.1->seisbench) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->obspy>=1.3.1->seisbench) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->obspy>=1.3.1->seisbench) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->obspy>=1.3.1->seisbench) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->obspy>=1.3.1->seisbench) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3->obspy>=1.3.1->seisbench) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1->seisbench) (1.17.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<2->obspy>=1.3.1->seisbench) (3.2.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->seisbench) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->obspy>=1.3.1->seisbench) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->obspy>=1.3.1->seisbench) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->obspy>=1.3.1->seisbench) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->obspy>=1.3.1->seisbench) (2025.7.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install seisbench #  Load the main seisbench library           # to install the package of seisbench and pyocto"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90ad8e45",
      "metadata": {
        "id": "90ad8e45"
      },
      "source": [
        "*This cell is required to circumvent an issue with colab and obspy. For details, check this issue in the obspy documentation: https://github.com/obspy/obspy/issues/2547*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "841a16b1",
      "metadata": {
        "id": "841a16b1"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import obspy                                   # import seismological libraries\n",
        "    obspy.read()\n",
        "except TypeError:\n",
        "    # Needs to restart the runtime once, because obspy only works properly after restart.\n",
        "    print('Stopping RUNTIME. If you run this code for the first time, this is expected. Colaboratory will restart automatically. Please run again.')\n",
        "    exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2aa6b036",
      "metadata": {
        "id": "2aa6b036"
      },
      "source": [
        "# Creating a catalog - From waveforms to events\n",
        "\n",
        "This model shows how to use [SeisBench](https://github.com/seisbench/seisbench) and the [PyOcto associator](https://github.com/yetinam/pyocto) to create an earthquake catalog from raw waveforms. First, we will download raw waveforms and station metadata through FDSN. Second, we use an PhaseNet model in SeisBench to obtain phase arrival picks. Third, we use the PyOcto associator to associate the picks to events. We visualize the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "412d4fb1",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T22:15:58.581867Z",
          "start_time": "2023-12-08T22:15:57.347507Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "412d4fb1",
        "outputId": "dfe3602d-92e3-4a8a-b7b7-98219088df4f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pyocto'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-1369094122.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m                      \u001b[0;31m# to access Octo seismic event catalog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyocto\u001b[0m                    \u001b[0;31m# to access Octo seismic event catalog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseisbench\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msbm\u001b[0m          \u001b[0;31m# For loading ML models like PhaseNet, EQTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyocto'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "#Import all necessary libraries and modules:\n",
        "\n",
        "import obspy           # For seismology data (retrieval, waveform, event info)\n",
        "from obspy.clients.fdsn import Client\n",
        "from obspy import UTCDateTime\n",
        "import numpy as np             # numpy, Counter: Numerical and frequency analysis.\n",
        "\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt       # matplotlib, seaborn: For plotting results.\n",
        "import seaborn as sns\n",
        "import torch                      # to access Octo seismic event catalog\n",
        "\n",
        "import pyocto                    # to access Octo seismic event catalog\n",
        "import seisbench.models as sbm          # For loading ML models like PhaseNet, EQTransformer\n",
        "\n",
        "sns.set(font_scale=1.2)\n",
        "sns.set_style(\"ticks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "643e03e4",
      "metadata": {
        "id": "643e03e4"
      },
      "source": [
        "## Obtaining the data\n",
        "\n",
        "We download waveform data for 12 hours from the CX network in Northern Chile. We use the 5rd April 2014, about 72 hours after the Mw 8.1 Iquique mainshock late on 1st April 2014. Therefore, we expect to see high levels of aftershock activity. In addition to the waveform data, we download the station inventories. Downloading the data might take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79b4395a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T22:18:28.169667Z",
          "start_time": "2023-12-08T22:15:58.583161Z"
        },
        "id": "79b4395a"
      },
      "outputs": [],
      "source": [
        "# download 12 hours of waveform data from the CX network using the GFZ FDSN client.\n",
        "# The waveform data is stored in stream, and the station metadata in inv\n",
        "\n",
        "# client = Client(\"NOA\")           # Use NOA as FDSN data provider\n",
        "\n",
        "# Define start and end time for waveform download\n",
        "t0 = UTCDateTime(\"2011-02-07T12-00-00\")                        # Start time\n",
        "t1 = UTCDateTime(\"2011-02-07T23-59-59\")                        # End time =  the same day after 12 hours of recording\n",
        "\n",
        "# Optional: You could use 24 hours instead, but this requires more memory\n",
        "# t1 = t0 + 24 * 60 * 60   # Full day, requires more memory\n",
        "\n",
        "# Download waveform data from GFZ for network \"CX\", station \"*\", all locations and channels starting with \"HH\"\n",
        "#stream = client.get_waveforms(network=\"*\", station=station_str_husn, location=\"*\", channel=\"HH*\", starttime=t0, endtime=t1)\n",
        "\n",
        "# Get station metadata for the same query\n",
        "#inv = client.get_stations(network=\"*\", station=station_str_husn, location=\"*\", channel=\"HH*\", starttime=t0, endtime=t1)\n",
        "\n",
        "\n",
        "\n",
        "## Here, I download station data and meta-data from the Geophone Data Archive. These are the Nisyros stations. So they are available online\n",
        "client      = Client(\"GFZ\")  # or your specific FDSN service\n",
        "stations_nis    = [\"AMOE\",\"ANAF\", \"ASTY\",\"KOSE\",\"PSE1\",\"RHO1\",\"RHO2\",\"TIL3\",\"XALK\"]\n",
        "station_str_nis = \",\".join(stations_nis)\n",
        "\n",
        "stream_nis  = client.get_waveforms(network=\"6E\", station=station_str_nis, location=\"*\", channel=\"HH*\", starttime=t0, endtime=t1)\n",
        "inv_nis      = client.get_stations(network=\"6E\", station=station_str_nis, location=\"*\", channel=\"HH*\", starttime=t0, endtime=t1)\n",
        "\n",
        "## Here, I download station data and meta-data from the NOA Data Archive.\n",
        "client = Client('NOA')\n",
        "stations_husn    = [\"ARG\",\"APE\",\"ASTY\",\"LAST\",\"NISR\",\"NPS\", \"KARP\", \"ZKR\"]\n",
        "station_str_husn = \",\".join(stations_husn)\n",
        "\n",
        "stream_hsnu = client.get_waveforms(network=\"*\", station=station_str_husn, location=\"*\", channel=\"HH*\", starttime=t0, endtime=t1)\n",
        "inv_hsnu     = client.get_stations(network=\"*\", station=station_str_husn, location=\"*\", channel=\"HH*\", starttime=t0, endtime=t1)\n",
        "\n",
        "#%%\n",
        "\n",
        "## Here I merge the data and the meta data\n",
        "merged_inv = inv_nis + inv_hsnu\n",
        "merged_str  = stream_nis + stream_hsnu"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54e04a23",
      "metadata": {
        "id": "54e04a23"
      },
      "source": [
        "## Picking\n",
        "\n",
        "For this example, we use PhaseNet trained on the INSTANCE dataset from Italy. However, in principal any picker could be used for obtaining the picks with only minimal changes.\n",
        "\n",
        "**Warning:** This will take some time and requires sufficient main memory. If you are short on memory, reduce the study time in the cell before.\n",
        "\n",
        "**Note:** We automatically check if CUDA is available and run the model on CUDA in this case. Alternatively, the model runs on CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cd8f1f2",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T22:19:09.129111Z",
          "start_time": "2023-12-08T22:18:28.171400Z"
        },
        "id": "2cd8f1f2"
      },
      "outputs": [],
      "source": [
        "# Load the PhaseNet model and applies the model to the waveform stream with relatively low pick thresholds (0.05) to allow more picks.\n",
        "\n",
        "picker = sbm.PhaseNet.from_pretrained(\"instance\")           # Load pretrained PhaseNet model (trained on INSTANCE dataset)\n",
        "\n",
        "if torch.cuda.is_available():# Check if GPU is available\n",
        "    picker.cuda()              # Use GPU if available for faster processing\n",
        "\n",
        "# Run the picker on the waveform stream to identify P and S phases\n",
        "# batch_size=256: process waveforms in batches\n",
        "# P_threshold and S_threshold set the minimum probability to accept a pick\n",
        "\n",
        "# We tuned the thresholds a bit - Feel free to play around with these values\n",
        "picks = picker.classify(stream_nis, batch_size=256, P_threshold=0.05, S_threshold=0.05).picks\n",
        "#  PhaseNet is very sensitive due to low thresholds."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58ed9383",
      "metadata": {
        "id": "58ed9383"
      },
      "source": [
        "Let's have a look at the picks determined by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "215560d0",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T22:19:09.134133Z",
          "start_time": "2023-12-08T22:19:09.130147Z"
        },
        "scrolled": true,
        "id": "215560d0"
      },
      "outputs": [],
      "source": [
        "# Count how many picks were classified as P and S phases.\n",
        "# Print the total count and a preview of the pick list.\n",
        "\n",
        "counts = Counter([p.phase for p in picks])                  # Count how many P and S picks were made\n",
        "\n",
        "# Print summary\n",
        "print(\"P picks:\", counts[\"P\"], \"\\t\\tS picks:\", counts[\"S\"], \"\\n\")  # Output number of P and S picks\n",
        "\n",
        "# Print all picks: shows station, time, and phase (P or S)\n",
        "print(picks)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8c85b0a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T20:30:19.448209Z",
          "start_time": "2023-12-08T20:30:19.446178Z"
        },
        "id": "d8c85b0a"
      },
      "source": [
        "## Configuring the associator\n",
        "\n",
        "We now set up the associator. First, we define a velocity model to use. We go for a homogeneous velocity model. While a crude approximation in a subduction zone, it still gives good association. Check out the [PyOcto documentation](https://pyocto.readthedocs.io/en/latest/pages/velocity_models.html) for details on velocity models and how to use 1D models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d02967c5",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T22:19:09.142389Z",
          "start_time": "2023-12-08T22:19:09.135132Z"
        },
        "id": "d02967c5"
      },
      "outputs": [],
      "source": [
        "# Create a simple 0D velocity model (constant velocities) for associating P and S picks into seismic events.\n",
        "\n",
        "velocity_model = pyocto.VelocityModel0D(\n",
        "      p_velocity=7.0,                     # Assumed P-wave velocity in km/s\n",
        "    s_velocity=4.0,                     # Assumed S-wave velocity in km/s\n",
        "    tolerance=2.0,                      # Maximum residual time in seconds for associating picks into events\n",
        "    association_cutoff_distance=250,   # Max distance (in km) for stations to be grouped into the same event\n",
        ")\n",
        "\n",
        "\n",
        "# p_velocity, s_velocity: help estimate distances and origin times.\n",
        "# tolerance: allows some flexibility in phase arrival times.\n",
        "# association_cutoff_distance: max spatial distance between picks to consider them from the same event."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03a48350",
      "metadata": {
        "id": "03a48350"
      },
      "source": [
        "Next, we create the associator. We use the `from_area` function, as it will automatically select a local coordinate projection to use for the association. In addition, we set the overlap time (`time_before`), and pick based quality control criteria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8269857e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T22:19:09.159074Z",
          "start_time": "2023-12-08T22:19:09.143350Z"
        },
        "id": "8269857e"
      },
      "outputs": [],
      "source": [
        "# Create an OctoAssociator object using a geographic area specification.\n",
        "# The associator clusters phase picks into events based on spatial and temporal criteria.\n",
        "\n",
        "associator = pyocto.OctoAssociator.from_area(\n",
        "    lat=(34, 40),                 # Latitude bounds for the region of interest (Southern Peru/Northern Chile)\n",
        "    lon=(20, 30),                # Longitude bounds for the region of interest (Andes subduction zone)\n",
        "    zlim=(0, 200),                   # Depth limits in km for event association (0 to 200 km deep)\n",
        "    time_before=300,                # Look-back time window in seconds before each pick for possible event associations\n",
        "    velocity_model=velocity_model,  # Velocity model used for travel-time predictions (defined earlier)\n",
        "    n_picks=7,                     # Minimum number of total picks required to form an event\n",
        "    n_p_and_s_picks=3,              # Minimum number of picks that must include both P and S phases\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "235cfe40",
      "metadata": {
        "id": "235cfe40"
      },
      "source": [
        "Lastly, we convert the station information from the obspy inventory to the data frame required for PyOcto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f263bf7a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T22:19:09.164711Z",
          "start_time": "2023-12-08T22:19:09.160081Z"
        },
        "id": "f263bf7a"
      },
      "outputs": [],
      "source": [
        "# Convert Station Inventory to DataFrame\n",
        "# Convert ObsPy Inventory (station metadata) into a pandas DataFrame format required by PyOcto for associating picks with stations\n",
        "stations = associator.inventory_to_df(merged_inv)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "781fa162",
      "metadata": {
        "id": "781fa162"
      },
      "source": [
        "## Association\n",
        "\n",
        "We now run the phase association. We use the `associate_seisbench` function that directly takes the output of a SeisBench picking model as input. PyOcto offers further interfaces, for example, to input Pandas data frames. The association will take a moment (up to a few minutes on slower CPUs). The output are two dataframe:\n",
        "\n",
        "- `events` contains a list of all events with their location and origin times\n",
        "- `assignments` contains all picks and the event they are associated with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a01b4a82",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T22:19:48.225830Z",
          "start_time": "2023-12-08T22:19:09.165706Z"
        },
        "id": "a01b4a82"
      },
      "outputs": [],
      "source": [
        "# Run Phase Association with PyOcto that use both the pick information and station metadata\n",
        "# Run phase association using PyOcto’s built-in method for SeisBench pick format to:\n",
        "# 1- Estimate the origin time and hypocenter location of each seismic event.\n",
        "# 2- Associate each pick with a specific event.\n",
        "# This method links seismic picks into seismic events using clustering & travel time models\n",
        "\n",
        "events, assignments = associator.associate_seisbench(picks, stations)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79471dd8",
      "metadata": {
        "id": "79471dd8"
      },
      "source": [
        "The events currently only contain locations in a local coordinate system. Let's transform it back to latitude and longitude before inspecting the events."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "410d6c64",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T22:19:48.242663Z",
          "start_time": "2023-12-08T22:19:48.227287Z"
        },
        "id": "410d6c64"
      },
      "outputs": [],
      "source": [
        "# Convert local event coordinates (e.g., in meters) back to geographic coordinates (latitude/longitude)\n",
        "\n",
        "associator.transform_events(events);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff90c5ac",
      "metadata": {
        "id": "ff90c5ac"
      },
      "source": [
        "## Visualizing the catalog\n",
        "\n",
        "Let's have a look at the list of events. Every event is listed with its time, local coordinates (x, y, z), the number of picks and the latitude, longitude and depth. Within the 12 hours of the example, we found over 400 event, on average one event every 100 seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29f9de38",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T22:19:48.264092Z",
          "start_time": "2023-12-08T22:19:48.244259Z"
        },
        "id": "29f9de38"
      },
      "outputs": [],
      "source": [
        "# Visualizing the catalog and displaying the DataFrame events that was generated by the association process in Cell 10 and converted to lat/lon in Cell 11\n",
        "\n",
        "events"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Here I plot the stations\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.set_aspect(\"equal\")\n",
        "ax.plot(stations[\"x\"], stations[\"y\"], \"r^\", ms=10, mew=1, mec=\"k\")\n",
        "ax.set_xlabel(\"Easting [km]\")\n",
        "ax.set_ylabel(\"Northing [km]\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SUvAxbCDYNcO"
      },
      "id": "SUvAxbCDYNcO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a112c365",
      "metadata": {
        "id": "a112c365"
      },
      "source": [
        "We can also plot the catalog. Conveniently, we already have a local transverse mercator projection available, so no need for further thought in the plotting here. We use the `scatter` function and encode the depth of the events using color. The plot nicely resolves the intense shallow aftershock activity. It also shows the seismicity along the Slap (West-East dipping)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50af8a15",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T22:19:48.519146Z",
          "start_time": "2023-12-08T22:19:48.265304Z"
        },
        "id": "50af8a15"
      },
      "outputs": [],
      "source": [
        "# Create a new figure with specified size (10x10 inches)\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "\n",
        "# Add a subplot to the figure (1x1 grid, position 1)\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "# Set the aspect ratio of the plot to \"equal\" so that units on both axes are equally scaled\n",
        "ax.set_aspect(\"equal\")\n",
        "\n",
        "# Create a scatter plot of earthquake events\n",
        "# x and y coordinates are taken from the projected event positions\n",
        "# Color of each point represents the depth ('z') of the earthquake\n",
        "# Size of each point is 8, and the color map used is 'viridis' (dark = shallow, bright = deep)\n",
        "cb = ax.scatter(events[\"x\"], events[\"y\"], c=events[\"z\"], s=8, cmap=\"viridis\")\n",
        "\n",
        "# Add a colorbar to the figure, linked to the scatter plot\n",
        "cbar = fig.colorbar(cb)\n",
        "\n",
        "# Reverse the colorbar so that shallow events (small depth values) are on top\n",
        "cbar.ax.set_ylim(cbar.ax.get_ylim()[::-1])\n",
        "\n",
        "# Label the colorbar to indicate it shows event depth in kilometers\n",
        "cbar.set_label(\"Depth [km]\")\n",
        "\n",
        "# Plot the station locations using red upward-pointing triangles\n",
        "# Size of each triangle is 10, with a black edge (mec = marker edge color)\n",
        "ax.plot(stations[\"x\"], stations[\"y\"], \"r^\", ms=10, mew=1, mec=\"k\")\n",
        "\n",
        "# Label the x-axis as Easting (in kilometers)\n",
        "ax.set_xlabel(\"Easting [km]\")\n",
        "\n",
        "# Label the y-axis as Northing (in kilometers)\n",
        "ax.set_ylabel(\"Northing [km]\")\n",
        "\n",
        "# Display the full plot\n",
        "plt.show()\n",
        "\n",
        "# Output : #  Many earthquakes are clustered in the upper-left region.\n",
        "           # Most earthquakes are shallow (dark-colored).\n",
        "           # The stations are spread across the region, helping record the events."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b598ad17",
      "metadata": {
        "id": "b598ad17"
      },
      "source": [
        "As a last check, we manually inspect some events. The code block below selects a random event and plots the waveforms, together with the P (solid black lines) and S (dashed black lines). The x axis denotes the time, the y axis the distance between station and estimated event location. Therefore, we should see roughly a hyperbolic moveout. Run the cell a few times to see a few example events."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01622b7c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T22:19:48.744687Z",
          "start_time": "2023-12-08T22:19:48.520492Z"
        },
        "id": "01622b7c"
      },
      "outputs": [],
      "source": [
        "# Select a random event and plots the waveforms\n",
        "\n",
        "# Pick a random event index from the list of associated events\n",
        "event_idx = np.random.choice(events[\"idx\"])\n",
        "\n",
        "# Select all picks that were assigned to this specific event\n",
        "event_picks = [picks[i] for i in assignments[assignments[\"event_idx\"] == event_idx][\"pick_idx\"]]\n",
        "\n",
        "# Extract the event metadata (coordinates, time, etc.)\n",
        "event = events[events[\"idx\"] == event_idx].iloc[0]\n",
        "\n",
        "# Create a dictionary mapping station IDs to their (x, y) coordinates for easy lookup\n",
        "station_dict = {station: (x, y) for station, x, y in zip(stations[\"id\"], stations[\"x\"], stations[\"y\"])}\n",
        "\n",
        "# Find the earliest and latest pick time for this event\n",
        "first, last = min(pick.peak_time for pick in event_picks), max(pick.peak_time for pick in event_picks)\n",
        "\n",
        "# Create an empty ObsPy Stream to hold waveform traces\n",
        "sub = obspy.Stream()\n",
        "\n",
        "# Loop over all unique station IDs in the event picks\n",
        "for station in np.unique([pick.trace_id for pick in event_picks]):\n",
        "    # Select the vertical component (HHZ) waveform from the original stream\n",
        "    sub.append(merged_str.select(station=station[3:-1], channel=\"HHZ\")[0])\n",
        "\n",
        "# Slice the waveform data to a time window around the event (±5s from pick times)\n",
        "sub = sub.slice(first - 5, last + 5)\n",
        "\n",
        "# Copy, detrend, and high-pass filter the waveforms to remove noise and offsets\n",
        "sub = sub.copy()\n",
        "sub.detrend()\n",
        "sub.filter(\"highpass\", freq=2)\n",
        "\n",
        "# Set up the plot\n",
        "fig = plt.figure(figsize=(15, 10))\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "# Plot each waveform trace\n",
        "for i, trace in enumerate(sub):\n",
        "    normed = trace.data - np.mean(trace.data)                   # Remove DC offset\n",
        "    normed = normed / np.max(np.abs(normed))                    # Normalize amplitude\n",
        "    station_x, station_y = station_dict[trace.id[:-4]]          # Get station location\n",
        "    # Calculate the hypocentral distance from station to event\n",
        "    y = np.sqrt((station_x - event[\"x\"]) ** 2 + (station_y - event[\"y\"]) ** 2 + event[\"z\"] ** 2)\n",
        "    # Plot the waveform trace, shifted vertically by its hypocentral distance\n",
        "    ax.plot(trace.times(), 10 * normed + y)\n",
        "\n",
        "# Overlay P and S picks as vertical lines at their time and distance\n",
        "for pick in event_picks:\n",
        "    station_x, station_y = station_dict[pick.trace_id]\n",
        "    y = np.sqrt((station_x - event[\"x\"]) ** 2 + (station_y - event[\"y\"]) ** 2 + event[\"z\"] ** 2)\n",
        "    x = pick.peak_time - trace.stats.starttime                   # Pick time relative to trace start\n",
        "    ls = '-' if pick.phase == \"P\" else '--'                     # Line style: solid for P, dashed for S\n",
        "    ax.plot([x, x], [y - 10, y + 10], 'k', ls=ls)               # Plot pick as vertical line\n",
        "\n",
        "# Configure plot appearance\n",
        "ax.set_ylim(0)                                                 # Set lower limit of y-axis\n",
        "ax.set_xlim(0, np.max(trace.times()))                          # Set x-axis from 0 to max time\n",
        "ax.set_ylabel(\"Hypocentral distance [km]\")                     # Label y-axis\n",
        "ax.set_xlabel(\"Time [s]\")                                      # Label x-axis\n",
        "\n",
        "# Print out basic event information in the notebook\n",
        "print(\"Event information\")\n",
        "print(event)\n",
        "\n",
        "# Output: One seismic event, recorded at multiple stations (as seismograms/traces).\n",
        "# Each colored waveform is a seismogram from a station that detected the earthquake\n",
        "           # The black vertical lines show the detected seismic phases:\n",
        "                         # Solid lines = P-wave arrivals (faster, arrive first)\n",
        "                         # Dashed lines = S-wave arrivals (slower, arrive later)\n",
        "# Waveforms from stations closer to the event appear lower on the plot.\n",
        "# The pattern of wave arrival times increasing with distance, forming a hyperbolic moveout\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0cf477a",
      "metadata": {
        "id": "e0cf477a"
      },
      "source": [
        "## Closing remarks\n",
        "\n",
        "In this tutorial, we saw how to quickly generate an event catalog from raw seismic waveforms and their metadata using SeisBench and the PyOcto associator.\n",
        "\n",
        "- Both the picker and the associator have several tuning parameters. We tuned these parameters loosely, but we would like to make the reader aware that these parameters can have strong influence on the number of picks and events, the number of false positives, and the runtime performance of the associator. To learn more about PyOcto, check out the [Github repository](https://github.com/yetinam/pyocto) and the [documentation](https://pyocto.readthedocs.io/en/latest/index.html).\n",
        "- While PyOcto outputs locations, these are only preliminary. They will usually not be as accurate as locations determined with dedicated tools, such as NonLinLoc. It is highly recommended to follow-up the event detection with a dedicated location procedure."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}